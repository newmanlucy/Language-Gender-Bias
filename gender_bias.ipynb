{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gender bias.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaTLgRNCSoQc"
      },
      "source": [
        "# Gender Bias in Common Adjectives\n",
        "\n",
        "Inspired by [this](https://arxiv.org/pdf/1607.06520.pdf) article linked to in the New York Times this morning, I decided to do some experiments on how gender bias influences words that are not intended to connote gender. \n",
        "\n",
        "## Algebra with Words\n",
        "\n",
        "Words can be represented as abstractly as vectors in a 300-dimensional space. The attributes are determined by a machine learning algorithm, so they don't mean much by themselves. But this allows you, loosely speaking, to do algebra on words: King - Man + Woman = Queen.\n",
        "\n",
        "What we're really doing here is making an analogy. \"Man is to king as woman is to...?\" And we want the computer to be able to return \"Queen\" as the answer.\n",
        "\n",
        "So I defined an analogy function using the Python `gensim` library, which allows you to easily make such comparisons based on a model pre-trained on 100 billion words of Google News data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZUONxhHTco8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "00da0318-b3b5-4c70-e0f3-aaa897a8e428"
      },
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')\n",
        "\n",
        "def analogy(w1a, w1b, w2a):\n",
        "\tresult = wv.most_similar(negative=[w1a], \n",
        "                                positive=[w1b, w2a])\n",
        "\tif result[0][0].lower() == w1b:\n",
        "\t\treturn result[1]\n",
        "\treturn result[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[================================================--] 97.7% 1623.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybWhnnXnWc1h"
      },
      "source": [
        "## Bias Experiments\n",
        "\n",
        "The article I linked to above talks about detecting gender bias in words that shouldn't, by definition, be related to gender. It also presents a method for removing such bias.\n",
        "\n",
        "I decided to do some of my own experiments looking at the way gender bias is presented in adjectives that can be used to describe a person of any gender. For each word, `word`, in my list, I looked at the result of `analogy(\"he\", word, \"she\")` and the result of `analogy(\"she\", word, \"he\")`. The former is like \"adding femininity\" to the word, while the latter is like \"adding masculinity.\"\n",
        "\n",
        "Some of the words I used came up with non-sensical results, and others came up with very similar results, indicating little gender bias in the word. The word list I will discuss here is: \"badass\", \"sexy\", \"affectionate\", \"beautiful\",\"anxious\", \"overbearing\", \"assertive\", \"bossy\", \"rude\", \"fixated\", \"unhappy\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q23cV6lhZx3-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "36878ee8-0746-43d4-a294-224b8a9589df"
      },
      "source": [
        "\twords = [\"badass\", \"sexy\", \"affectionate\", \"beautiful\", \"anxious\", \"overbearing\", \"assertive\", \"bossy\", \"rude\", \"fixated\", \"unhappy\"]\n",
        "\tfor word in words:\n",
        "\t\tfemale_result = analogy(\"he\", word, \"she\")\n",
        "\t\tmale_result = analogy(\"she\", word, \"he\")\n",
        "\t\tprint(\"He:%s :: she:%s\" % (word, female_result[0]))\n",
        "\t\tprint(\"She:%s :: he:%s\" % (word, male_result[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "He:badass :: she:superheroine\n",
            "She:badass :: he:dude\n",
            "He:sexy :: she:sassy\n",
            "She:sexy :: he:suave\n",
            "He:affectionate :: she:motherly\n",
            "She:affectionate :: he:genial\n",
            "He:beautiful :: she:gorgeous\n",
            "She:beautiful :: he:magnificent\n",
            "He:anxious :: she:fearful\n",
            "She:anxious :: he:eager\n",
            "He:overbearing :: she:bossy\n",
            "She:overbearing :: he:arrogant\n",
            "He:assertive :: she:bossy\n",
            "She:assertive :: he:forceful\n",
            "He:bossy :: she:bitchy\n",
            "She:bossy :: he:arrogant\n",
            "He:rude :: she:catty\n",
            "She:rude :: he:discourteous\n",
            "He:fixated :: she:obsessed\n",
            "She:fixated :: he:preoccupied\n",
            "He:unhappy :: she:dissatisfied\n",
            "She:unhappy :: he:displeased\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGPGnloSZ7Jm"
      },
      "source": [
        "## Results\n",
        "\n",
        "Below is a graphic with some of the results. \n",
        "\n",
        "![Gender Bias Results](https://i.imgur.com/tRnB789.jpg)\n",
        "\n",
        "## Discussion\n",
        "\n",
        "First, these results should be taken with some healthy grains of salt. I didn't consider the strength of the analogies at all, and I cherry-picked the results I found most interesting. These relationships are based on the Google News training dataset, which is not all-encompassing. \n",
        "\n",
        "That being said, some observations:\n",
        "\n",
        "* The model thinks female + badass is a superheroine, while a male + badass is just a dude, implying that the requirements for being \"badass\" are harder to meet as a woman. \n",
        "* If you take \"affectionate\" in the female direction, you get a word tied by dictionary definition to womanhood. If you take it in the male direction, you get a word with a signficantly less warm connotation.\n",
        "* The model seems to be using a different definition of \"anxious\" for the female and male versions. Women get the definition associated with a mental health disorder, while men get the a positive, desirable attribute. This associaton of anxiety with feminitiy may be harmful for men who experience anxiety.\n",
        "* For words like \"bossy\" and \"rude\", the female version is decidedly more insulting. For women, there are fine lines between \"assertive,\" \"bossy,\" and \"bitchy.\"\n",
        "* For the word \"unhappy,\" the feminized version \"dissatisfied\" is more passive than the masculinized version, \"displeased.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6co1TLTZ0hR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}